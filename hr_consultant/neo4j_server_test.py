import os
import json
from dotenv import load_dotenv

# --- 1. å¯¼å…¥ FastMCP å’Œ LangChain ç»„ä»¶ ---
from mcp.server.fastmcp import FastMCP
from langchain_community.vectorstores import Neo4jVector
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# neo4j_lcel_server.py
from openai import OpenAI
# from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.embeddings.dashscope import DashScopeEmbeddings
# æˆ–è€…ï¼Œåœ¨æŸäº›æœ€æ–°ç‰ˆæœ¬ä¸­ï¼Œå®ƒå¯èƒ½ç›´æ¥æ˜¯:
# from langchain_dashscope import DashscopeEmbeddings
from langchain_neo4j import Neo4jGraph

load_dotenv(dotenv_path="/Users/dengjuan1/build_agent/.env")

# --- 2. åˆ›å»º FastMCP å®ä¾‹ ---
mcp = FastMCP("knowledge_graph_agent")

# --- 3. å…¨å±€/åº”ç”¨ä¸Šä¸‹æ–‡çš„çŠ¶æ€ç®¡ç† ---
# å› ä¸ºå‡½æ•°æ˜¯æ— çŠ¶æ€çš„ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåœ°æ–¹æ¥æŒæœ‰æ˜‚è´µçš„ã€å¯å¤ç”¨çš„ RAG é“¾å¯¹è±¡ã€‚
# ä¸€ä¸ªç®€å•çš„å­—å…¸æˆ–ä¸€ä¸ªä¸“é—¨çš„ç±»å®ä¾‹éƒ½å¯ä»¥ã€‚
app_context = {}


@mcp.tool()
def setup_knowledge_graph_index() -> str:
    """
    Checks if the knowledge graph vector index exists. If not, it generates embeddings for all employees and creates the index.
    This is a one-time setup or maintenance tool.
    """
    kg = app_context["kg"]
    embeddings = app_context["embeddings"]
    index_name = "employee_embeddings"

    # æ­¥éª¤ 1: æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    print(f"Checking for index '{index_name}'...")
    indexes_info = kg.query(f"SHOW VECTOR INDEXES WHERE name = '{index_name}'")

    if indexes_info:
        message = f"Index '{index_name}' already exists. No action needed."
        print(message)
        return json.dumps({"status": "success", "message": message, "details": indexes_info[0]})

    # kg.query(f"DROP INDEX {index_name}")

    # æ­¥éª¤ 2: å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ™ç”Ÿæˆ Embeddings
    print(f"Index not found. Starting embedding process for employees...")

    # ä» Neo4j ä¸­æå–éœ€è¦ embedding çš„æ•°æ®
    query = "MATCH (e:employee) WHERE e.bio IS NOT NULL RETURN e.employee_name AS name, e.bio AS bio"
    results = kg.query(query)

    if not results:
        message = "No employees with 'bio' property found to embed."
        print(message)
        return json.dumps({"status": "skipped", "message": message})

    embedded_count = 0
    for record in results:
        name, bio = record["name"], record["bio"]
        if bio:
            print(f"ğŸ“ Embedding for: {name}")
            # ä½¿ç”¨ TongyiEmbeddings ç”Ÿæˆå‘é‡
            embedding_vector = embeddings.embed_query(bio)

            # å°†å‘é‡å†™å› Neo4j
            update_query = """
            MATCH (e:employee {employee_name: $name})
            SET e.bioEmbedding = $embedding
            """
            kg.query(update_query, params={"name": name, "embedding": embedding_vector})
            print(f"âœ… Stored embedding for: {name}")
            embedded_count += 1

    # æ­¥éª¤ 3: åˆ›å»ºå‘é‡ç´¢å¼•
    print(f"Creating vector index '{index_name}' with dimension 1536 for Qwen...")
    kg.query(f"""
      CREATE VECTOR INDEX {index_name} IF NOT EXISTS
      FOR (e:employee) ON (e.bioEmbedding) 
      OPTIONS {{ indexConfig: {{
        `vector.dimensions`: 1536,
        `vector.similarity_function`: 'cosine'
      }}
    }}""")

    message = f"Successfully embedded {embedded_count} employees and created index '{index_name}'."
    print(message)
    return json.dumps({"status": "success", "message": message})

@mcp.tool()
def initialize_rag_chain():
    """
    åˆå§‹åŒ– LangChain RAG é“¾å¹¶å°†å…¶å­˜å‚¨åœ¨å…¨å±€ä¸Šä¸‹æ–‡ä¸­ã€‚
    è¿™ä¸ªå‡½æ•°åªä¼šåœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚
    """
    if app_context.get("rag_chain"):
        return

    print("Initializing Neo4j LCEL RAG Chain...")

    app_context["kg"] = Neo4jGraph(
        url=os.getenv("NEO4J_URI"),
        username=os.getenv("NEO4J_USERNAME"),
        password=os.getenv("NEO4J_PASSWORD"),
        database=os.getenv("NEO4J_DATABASE"),
    )
    # åˆå§‹åŒ–ç»„ä»¶ (å’Œä¹‹å‰å®Œå…¨ä¸€æ ·)
    app_context["embeddings"] = DashScopeEmbeddings(
        dashscope_api_key=os.getenv("DASHSCOPE_API_KEY"),
        model="text-embedding-v2",
    )

    employee_vector_store = Neo4jVector.from_existing_index(
        embedding=app_context["embeddings"],
        url=os.getenv("NEO4J_URI"),
        username=os.getenv("NEO4J_USERNAME"),
        password=os.getenv("NEO4J_PASSWORD"),
        database=os.getenv("NEO4J_DATABASE"),
        index_name="employee_embeddings",
    )

    retriever = employee_vector_store.as_retriever()

    llm = ChatOpenAI(
        model="deepseek-chat",
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        openai_api_base=os.getenv("DEEPSEEK_API_BASE"),
        temperature=0
    )

    def format_docs(docs):
        formatted_docs = []
        for doc in docs:
            employee_name = doc.metadata.get('employee_name', 'Unknown Employee')
            formatted_doc = f"Employee Name: {employee_name}\nBio: {doc.page_content}"
            formatted_docs.append(formatted_doc)
        return "\n\n".join(formatted_docs)

    template = """You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context to answer the question. 
    If you don't know the answer, just say that you don't know. 
    When you answer, you MUST mention the name of the employee who is the expert.

    Context:
    {context}
    
    Question: {question}
    
    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)

    # æ„å»ºå¹¶å­˜å‚¨ RAG é“¾
    rag_chain = (
            {"context": retriever | RunnableLambda(format_docs), "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    app_context["rag_chain"] = rag_chain
    print("Neo4j LCEL RAG Chain initialized successfully.")


# --- 4. ä½¿ç”¨ @mcp.tool() è£…é¥°å™¨æš´éœ²å·¥å…· ---

@mcp.tool()
def ask_knowledge_graph(question: str) -> str:
    """
    Answers questions about employees, their skills, and expertise by querying a knowledge graph.
    Use this for questions like 'Who is an expert in data science?' or 'Tell me about Michael's background'.
    """
    print(f"Invoking RAG chain with question: '{question}'")

    # ä»ä¸Šä¸‹æ–‡ä¸­è·å–å·²åˆå§‹åŒ–çš„ RAG é“¾
    rag_chain = app_context.get("rag_chain")
    if not rag_chain:
        # è¿™æ˜¯ä¸€ä¸ªå¤‡ç”¨å®‰å…¨æªæ–½ï¼Œæ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šæ‰§è¡Œ
        return json.dumps({"error": "RAG chain is not initialized."})

    try:
        # è°ƒç”¨ RAG é“¾
        answer = rag_chain.invoke(question)
        print(f"RAG chain returned answer: '{answer}'")
        # FastMCP ä¼šè‡ªåŠ¨å¤„ç†è¿”å›çš„å­—ç¬¦ä¸²
        return answer
    except Exception as e:
        print(f"Error invoking RAG chain: {e}")
        return json.dumps({"error": str(e)})


if __name__ == "__main__":
    initialize_rag_chain()
    mcp.run(transport='stdio')