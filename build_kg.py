import sys

import httpx
from dotenv import load_dotenv
import os
from openai import OpenAI
from langchain_community.graphs import Neo4jGraph
#%%

# Warning control
import warnings
warnings.filterwarnings("ignore")

load_dotenv('.env', override=True)
print("NEO4J_URI =", os.getenv('NEO4J_URI'))
#%%
#æ•°æ®åº“è¿æ¥
NEO4J_URI = os.getenv('NEO4J_URI')
NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')
NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')
NEO4J_DATABASE = os.getenv('NEO4J_DATABASE')

kg = Neo4jGraph(
    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE
)
#%%
#è¿™éƒ¨åˆ†æ˜¯cypher test
cypher1 = """
  MATCH (n) 
  RETURN count(n) AS numberOfNodes
  """

result_node = kg.query(cypher1)
print(f"There are {result_node[0]['numberOfNodes']} nodes in this graph.")

cypher2 = """
  MATCH (n:employee) 
  RETURN count(n) AS numberOfEmployees
  """

result_employee = kg.query(cypher2)
print(f"There are {result_employee[0]['numberOfEmployees']} employee nodes in this graph.")

cypher3="""
  MATCH (Alice:employee {employee_name:"Alice"}) 
  RETURN Alice
  """
result_alice = kg.query(cypher3)
print(f"Alice info: {result_alice[0]} ")

cypher4="""
  MATCH (Alice:employee {employee_name:"Alice"}) 
  RETURN Alice.employee_name as name
  """
result_alice_name = kg.query(cypher4)
print(f"Alice name: {result_alice_name[0]['name']} ")

cypher5="""
  MATCH (employee:employee)-[:works_at]->(department:department) 
  RETURN employee.employee_name as name
  """
result_employee2 = kg.query(cypher5)
print(result_employee2)

cypher6="""
  MATCH (employee:employee)-[:works_at]->(eng_department:department {department_name:"Engineering"}) 
  RETURN employee.employee_name as name
  """
result_employee3 = kg.query(cypher6)
print(result_employee3)

#%%
#ä¸‹é¢æˆ‘ä»¬éœ€è¦è®©LLMå­¦ä¼šåŒ¹é…æˆ‘ä»¬Graphï¼Œæˆ‘ä»¬å°±éœ€è¦Embeddingæ¨¡å‹æ¥åŠ å…¥
# åµŒå…¥æ¨¡å‹åç§°ï¼ˆ1536ç»´åº¦ï¼‰
import google.generativeai as genai
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# EMBED_MODEL = "text-embedding-3-small"

# å…³é”®ä»£ç ï¼šåˆ›å»ºä¸€ä¸ªæ˜ç¡®ä¸ä½¿ç”¨ä»»ä½•ä»£ç†çš„httpxå®¢æˆ·ç«¯
client_with_no_proxy = httpx.Client(proxy=None)

# å°†è¿™ä¸ªå®¢æˆ·ç«¯ä¼ é€’ç»™gemini
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

def get_embedding_gemini(text, model="models/embedding-001", task_type='RETRIEVAL_QUERY'):
    # 2. è°ƒç”¨Googleçš„embed_contentæ–¹æ³•
    response = genai.embed_content(
        model=model,  # ä½¿ç”¨Googleçš„Embeddingæ¨¡å‹å
        content=text,
        task_type=task_type
    )
    # 3. æå–embeddingå‘é‡çš„æ–¹å¼ç•¥æœ‰ä¸åŒ
    return response['embedding']

def embed_employees_with_kg(kg: Neo4jGraph):
    """ä» Neo4j ä¸­æå– bioï¼Œç”Ÿæˆ embedding å¹¶å†™å›"""
    query = "MATCH (e:employee) RETURN e.employee_name AS name, e.bio AS bio"
    results = kg.query(query)

    for record in results:
        name = record["name"]
        bio = record["bio"]
        if bio:
            print(f"ğŸ“ æ­£åœ¨å¤„ç†ï¼š{name}")
            embedding = get_embedding_gemini(bio,task_type="RETRIEVAL_DOCUMENT")
            update_query = """
            MATCH (e:employee {employee_name: $name})
            SET e.bioEmbedding = $embedding
            """
            kg.query(update_query, params={"name": name, "embedding": embedding})
            print(f"âœ… å·²å†™å…¥ embeddingï¼š{name}")


embed_employees_with_kg(kg)

kg.query("""
  CREATE VECTOR INDEX employee_embeddings IF NOT EXISTS
  FOR (e:employee) ON (e.bioEmbedding) 
  OPTIONS { indexConfig: {
    `vector.dimensions`: 768,
    `vector.similarity_function`: 'cosine'
  }}"""
)
#%%
#ç›¸ä¼¼åº¦è®¡ç®—ï¼šæˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†åµŒå…¥å‘é‡ï¼Œç°åœ¨æˆ‘ä»¬æ¥æå‡ºé—®é¢˜
# --- è¿™æ˜¯æ‚¨æ–°çš„ç›¸ä¼¼åº¦æœç´¢å‡½æ•° ---
kg = Neo4jGraph(
    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE
)
def get_embedding_gemini(text, model="models/embedding-001", task_type='RETRIEVAL_QUERY'):
    # 2. è°ƒç”¨Googleçš„embed_contentæ–¹æ³•
    response = genai.embed_content(
        model=model,  # ä½¿ç”¨Googleçš„Embeddingæ¨¡å‹å
        content=text,
        task_type=task_type
    )
    # 3. æå–embeddingå‘é‡çš„æ–¹å¼ç•¥æœ‰ä¸åŒ
    return response['embedding']
def find_similar_movies(question, top_k=5):
    # æ­¥éª¤ä¸€ï¼šåœ¨Pythonä¸­ä¸ºç”¨æˆ·é—®é¢˜ç”ŸæˆEmbedding
    print(f"æ­£åœ¨ä¸ºé—®é¢˜ '{question}' ç”ŸæˆGemini Embedding...")
    question_embedding = get_embedding_gemini(question, task_type="RETRIEVAL_QUERY")

    # æ­¥éª¤äºŒï¼šå°†ç”Ÿæˆçš„å‘é‡ä½œä¸ºå‚æ•°ä¼ å…¥CypheræŸ¥è¯¢
    print(f"ä½¿ç”¨ç”Ÿæˆçš„å‘é‡åœ¨Neo4jä¸­è¿›è¡Œç›¸ä¼¼åº¦æœç´¢...")

    # å…³é”®ï¼šCypheræŸ¥è¯¢ä¸å†è°ƒç”¨genai.vector.encodeï¼Œè€Œæ˜¯ç›´æ¥æ¥æ”¶ä¸€ä¸ª$question_embeddingå‚æ•°
    cypher_query = """
        CALL db.index.vector.queryNodes(
            'employee_embeddings',  // <--- 1. ä½¿ç”¨æ‚¨ä¸ºGeminiæ•°æ®åˆ›å»ºçš„ç´¢å¼•å
            $top_k, 
            $question_embedding         // <--- 2. ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„å‘é‡å‚æ•°
        ) YIELD node AS employee, score
        MATCH (employee)-[:works_at]->(d:department)
        RETURN 
            employee.employee_name AS employeeName, 
            employee.bio AS bio, 
            d.department_name AS departmentName, // <-- è¿”å›éƒ¨é—¨åç§°
            score
    """

    # å‡†å¤‡è¦ä¼ å…¥çš„å‚æ•°
    params = {
        "question_embedding": question_embedding,  # <--- 3. å°†Pythonä¸­çš„å‘é‡å˜é‡ä¼ ç»™Cypherå‚æ•°
        "top_k": top_k
    }

    # æ‰§è¡ŒæŸ¥è¯¢
    results = kg.query(cypher_query, params=params)

    return results


# --- è°ƒç”¨ç¤ºä¾‹ ---
my_question = "A human resource professional"
similar_movies = find_similar_movies(my_question)

#%%
#åŠ å…¥langchainæ¥è‡ªåŠ¨ç¼–æ’ä»¥ä¸Šçš„apiè°ƒç”¨
# Langchain
from langchain_community.graphs import Neo4jGraph
from langchain_community.vectorstores import Neo4jVector
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain
from langchain_openai import ChatOpenAI

# å®šä¹‰å‚æ•°
INDEX_NAME = "employee_embeddings"  # æ‚¨ä¸ºGeminiæ•°æ®åˆ›å»ºçš„å‘é‡ç´¢å¼•å
NODE_LABEL = "employee"                      # èŠ‚ç‚¹çš„æ ‡ç­¾ (æ³¨æ„å¤§å°å†™ï¼Œé€šå¸¸é¦–å­—æ¯å¤§å†™)
TEXT_PROPERTY = "bio"                      # åŒ…å«æºæ–‡æœ¬çš„å±æ€§å
EMBEDDING_PROPERTY = "bioEmbedding"        # åŒ…å«å‘é‡çš„å±æ€§å
gemini_embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY") # <-- å…³é”®å°±åœ¨è¿™ä¸€è¡Œ
)
# ä½¿ç”¨ from_existing_graph åˆ›å»ºå‘é‡å­˜å‚¨å¯¹è±¡
employee_vector_store = Neo4jVector.from_existing_graph(
    embedding=gemini_embeddings,          # <-- ä½¿ç”¨Gemini Embeddingæ¨¡å‹
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
    index_name=INDEX_NAME,                # <-- å¡«å…¥æ­£ç¡®çš„ç´¢å¼•å
    node_label=NODE_LABEL,                # <-- å¡«å…¥æ­£ç¡®çš„èŠ‚ç‚¹æ ‡ç­¾
    text_node_properties=[TEXT_PROPERTY], # <-- è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå¡«å…¥æºæ–‡æœ¬å±æ€§å
    embedding_node_property=EMBEDDING_PROPERTY, # <-- å¡«å…¥å‘é‡å±æ€§å
)
#%%
#å¼€å§‹é—®ç­”
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI

# 1. å°†å‘é‡å­˜å‚¨åŒ…è£…æˆæ£€ç´¢å™¨
retriever = employee_vector_store.as_retriever()

import google.generativeai as genai

# é…ç½®æ‚¨çš„API Key
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

print("--- å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ ---")
for m in genai.list_models():
  # æ£€æŸ¥è¿™ä¸ªæ¨¡å‹æ˜¯å¦æ”¯æŒ 'generateContent' æ–¹æ³•
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

# 2. åˆå§‹åŒ–ä¸€ä¸ªèŠå¤©æ¨¡å‹ (æ¯”å¦‚Gemini Pro)
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")

# 3. æ„å»ºé—®ç­”é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff", # æˆ–è€… "map_reduce", "refine"
    retriever=retriever,
    return_source_documents=True # å»ºè®®å¼€å¯ï¼Œæ–¹ä¾¿æŸ¥çœ‹æ£€ç´¢åˆ°äº†å“ªäº›å†…å®¹
)

# 4. è¿›è¡Œæé—®ï¼
question = "Who is an expert in data science and graph technology?"
result = qa_chain.invoke({"query": question})

print("--- Answer ---")
print(result['result'])
#%%
#ä¸Šé¢çš„è¿™ä¸ªå›ç­”å·®å¼ºäººæ„ï¼Œæˆ‘ä»¬å¸Œæœ›å›ç­”ä¸­å¯ä»¥åŒ…å«äººå
#æ–¹å¼1:è®©llmå»å†™cypherå›ç­”é—®é¢˜
#å¤±è´¥ï¼šæœ¬è´¨ä¸Šå¯ä»¥ï¼Œä½†æ˜¯æˆ‘æ²¡æœ‰å®‰è£…genai
from langchain.chains import GraphCypherQAChain
from langchain_community.graphs import Neo4jGraph

# 1. åˆ›å»ºä¸€ä¸ªNeo4jGraphçš„å®ä¾‹ï¼Œå®ƒçŸ¥é“å›¾çš„schema
graph = Neo4jGraph(
    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD
)
llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")

# 2. åˆ›å»ºCypheré—®ç­”é“¾
cypher_qa_chain = GraphCypherQAChain.from_llm(
    graph=graph,
    llm=llm,
    verbose=True, # å¼€å¯verboseå¯ä»¥çœ‹åˆ°LLMç”Ÿæˆçš„Cypherè¯­å¥
    allow_dangerous_requests=True
    #ä½†æ˜¯è¿™ä¸ªè¡Œä¸ºéå¸¸å±é™©ï¼Œä¼šè§¦å‘langchainçš„å®‰å…¨æç¤ºï¼š
    #In order to use this chain, you must acknowledge that it can make dangerous requests by setting `allow_dangerous_requests` to `True`.
)

# 3. ç›´æ¥æé—®
question = "Who is an expert in data science and graph technology? Return their name."
result = cypher_qa_chain.invoke({"query": question})

print(result['result'])
#%%
#æ”¹Promptæ£€ç´¢meta
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import Neo4jVector
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# --- 1. åˆå§‹åŒ–ç»„ä»¶ (å’Œä¹‹å‰ä¸€æ ·) ---
employee_vector_store = Neo4jVector.from_existing_graph(
    embedding=gemini_embeddings,          # <-- ä½¿ç”¨Gemini Embeddingæ¨¡å‹
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD,
    index_name=INDEX_NAME,                # <-- å¡«å…¥æ­£ç¡®çš„ç´¢å¼•å
    node_label=NODE_LABEL,                # <-- å¡«å…¥æ­£ç¡®çš„èŠ‚ç‚¹æ ‡ç­¾
    text_node_properties=[TEXT_PROPERTY], # <-- è¿™æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå¡«å…¥æºæ–‡æœ¬å±æ€§å
    embedding_node_property=EMBEDDING_PROPERTY, # <-- å¡«å…¥å‘é‡å±æ€§å
)
retriever = employee_vector_store.as_retriever()

llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")

# --- 2. å®šä¹‰å¦‚ä½•æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ ---
# è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªDocumentåˆ—è¡¨ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–æˆæˆ‘ä»¬æƒ³è¦çš„å­—ç¬¦ä¸²
def format_docs(docs):
    formatted_docs = []
    for doc in docs:
        # ä»metadataä¸­æå–äººåï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ç”¨'Unknown'
        employee_name = doc.metadata.get('employee_name', 'Unknown Employee')
        # æ‹¼æ¥æˆæ¸…æ™°çš„æ ¼å¼
        formatted_doc = f"Employee Name: {employee_name}\nBio: {doc.page_content}"
        formatted_docs.append(formatted_doc)
    # ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦å°†æ‰€æœ‰æ ¼å¼åŒ–åçš„æ–‡æ¡£æ‹¼æ¥èµ·æ¥
    return "\n\n".join(formatted_docs)


# --- 3. åˆ›å»ºPromptæ¨¡æ¿ ---
# è¿™ä¸ªæ¨¡æ¿å’Œä¹‹å‰ä¸€æ ·ï¼Œä½†å®ƒæ›´ç¬¦åˆLCELçš„é£æ ¼
template = """
You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
When you answer, you MUST mention the name of the employee who is the expert.

Context:
{context}

Question: {question}

Answer:
"""
prompt = ChatPromptTemplate.from_template(template)


# --- 4. ä½¿ç”¨LCELæ„å»ºé—®ç­”é“¾ (æ ¸å¿ƒéƒ¨åˆ†) ---
rag_chain = (
    # è¿™ä¸€éƒ¨åˆ†å®šä¹‰äº†å¦‚ä½•å‡†å¤‡Promptçš„è¾“å…¥å˜é‡
    {
        "context": retriever | RunnableLambda(format_docs), # å…³é”®ï¼šå…ˆç”¨retrieveræ£€ç´¢ï¼Œç„¶åç”¨æˆ‘ä»¬çš„å‡½æ•°æ ¼å¼åŒ–ç»“æœ
        "question": RunnablePassthrough()                  # ç”¨æˆ·çš„åŸå§‹é—®é¢˜ç›´æ¥ä¼ é€’è¿‡å»
    }
    | prompt          # å°†å‡†å¤‡å¥½çš„å˜é‡å¡«å…¥Promptæ¨¡æ¿
    | llm             # å°†å¡«å……å¥½çš„Promptå‘é€ç»™LLM
    | StrOutputParser() # å°†LLMçš„è¾“å‡ºè§£æä¸ºå­—ç¬¦ä¸²
)


# --- 5. è°ƒç”¨é“¾å¹¶æé—® ---
question = "Who is an expert in data science and graph technology?"
answer = rag_chain.invoke(question) # LCELé“¾çš„è°ƒç”¨æ›´ç›´æ¥

print("--- Answer ---")
print(answer)